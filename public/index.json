[{"categories":["Growth Hacking"],"contents":"Comment créer un formulaire HTML qui stocke les données du formulaire soumis dans Google Sheets en utilisant le JavaScript (ES6), Google Apps Script, Fetch and FormData. 1. Créer une nouvelle fiche Google  D\u0026rsquo;abord, allez sur Google Sheets et lancez une nouvelle feuille de calcul avec le modèle vide. Renommez-le Email Subscribers. Ou peu importe, ça n\u0026rsquo;a pas d\u0026rsquo;importance. Mettez les en-têtes suivants dans la première ligne :      A B C \u0026hellip;     1 timestamp email       Pour apprendre comment ajouter des champs de saisie supplémentaires, section 7 ci-dessous.\n 2. Créer un script Google Apps  Cliquez sur \u0026ldquo;Outils \u0026gt; Editeur de script\u0026hellip;\u0026quot;, ce qui devrait ouvrir un nouvel onglet. Renommez-le Submit Form to Google Sheets.   Assurez-vous d\u0026rsquo;attendre qu\u0026rsquo;il enregistre et mette à jour le titre avant d\u0026rsquo;éditer le script.\n  Maintenant, supprimez le bloc function myFunction() {} dans l\u0026rsquo;onglet Code.gs. Collez le script suivant à sa place et \u0026ldquo;Fichier \u0026gt; Enregistrer\u0026rdquo; :  var sheetName = \u0026#39;Sheet1\u0026#39; //Attention au nom il est souvent traduit var scriptProp = PropertiesService.getScriptProperties() function intialSetup () { var activeSpreadsheet = SpreadsheetApp.getActiveSpreadsheet() scriptProp.setProperty(\u0026#39;key\u0026#39;, activeSpreadsheet.getId()) } function doPost (e) { var lock = LockService.getScriptLock() lock.tryLock(10000) try { var doc = SpreadsheetApp.openById(scriptProp.getProperty(\u0026#39;key\u0026#39;)) var sheet = doc.getSheetByName(sheetName) var headers = sheet.getRange(1, 1, 1, sheet.getLastColumn()).getValues()[0] var nextRow = sheet.getLastRow() + 1 var newRow = headers.map(function(header) { return header === \u0026#39;timestamp\u0026#39; ? new Date() : e.parameter[header] }) sheet.getRange(nextRow, 1, 1, newRow.length).setValues([newRow]) return ContentService .createTextOutput(JSON.stringify({ \u0026#39;result\u0026#39;: \u0026#39;success\u0026#39;, \u0026#39;row\u0026#39;: nextRow })) .setMimeType(ContentService.MimeType.JSON) } catch (e) { return ContentService .createTextOutput(JSON.stringify({ \u0026#39;result\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;error\u0026#39;: e })) .setMimeType(ContentService.MimeType.JSON) } finally { lock.releaseLock() } }  Si vous voulez mieux comprendre ce que fait ce script, consultez le fichier dans le repo pour une explication détaillée.\n form-script-commented.js\n3. Exécuter la fonction de configuration  Ensuite, allez à Exécuter \u0026gt; Exécuter la fonction \u0026gt; Configuration initiale pour exécuter cette fonction. Dans la boîte de dialogue Autorisation requise, cliquez sur Réviser les autorisations. Connectez-vous ou choisissez le compte Google associé à ce projet. Vous devriez voir un dialogue qui dit Hi {votre nom}, Soumettre le formulaire à Google Sheets veut\u0026hellip;   Cliquez sur Autoriser.\n 4. Ajouter un nouveau déclencheur de projet  Cliquez sur Editer \u0026gt; Déclencheurs du projet en cours. Dans la boîte de dialogue, cliquez sur Aucun déclencheur n'est configuré. Cliquez ici pour en ajouter un maintenant. Dans les listes déroulantes, sélectionnez doPost. Définissez les champs d\u0026rsquo;événements à De la feuille de calcul et Sur l'envoi du formulaire. Cliquez ensuite sur Sauvegarder.  5. Publier le projet sous forme d\u0026rsquo;application web  Cliquez sur Publish \u0026gt; Deploy as web app.... Définissez Version du projet sur Nouveau et mettez version initiale dans le champ de saisie ci-dessous. Laissez Exécuter l'application en tant que: défini sur Me(your@address.com). Pour Qui a accès à l'application: sélectionnez Anyone, even anonymous. Cliquez sur Déployer. Dans le popup, copiez l\u0026rsquo;URL de l\u0026rsquo;application Web actuelle à partir de la boîte de dialogue. Et cliquez sur OK.   IMPORTANT! Si vous avez un domaine personnalisé avec Gmail, vous devez peut-être cliquer sur \u0026ldquo;OK\u0026rdquo;, rafraîchir la page, puis aller sur Publier \u0026gt; Déployer en tant qu'application Web... pour obtenir l\u0026rsquo;URL de l\u0026rsquo;application Web appropriée. Cela devrait ressembler à quelque chose comme https://script.google.com/a/yourdomain.com/macros/s/XXXX….\n 6. Saisissez l\u0026rsquo;URL de votre application web Ouvrez le fichier nommé index.html. Sur la ligne 12, remplacez \u0026lt;SCRIPT URL\u0026gt; par l\u0026rsquo;url de votre script :\n\u0026lt;form name=\u0026#34;submit-to-google-sheet\u0026#34;\u0026gt; \u0026lt;input name=\u0026#34;email\u0026#34; type=\u0026#34;email\u0026#34; placeholder=\u0026#34;Email\u0026#34; required\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;script\u0026gt; const scriptURL = \u0026#39;\u0026lt;SCRIPT URL\u0026gt;\u0026#39; const form = document.forms[\u0026#39;submit-to-google-sheet\u0026#39;] form.addEventListener(\u0026#39;submit\u0026#39;, e =\u0026gt; { e.preventDefault() fetch(scriptURL, { method: \u0026#39;POST\u0026#39;, body: new FormData(form)}) .then(response =\u0026gt; console.log(\u0026#39;Success!\u0026#39;, response)) .catch(error =\u0026gt; console.error(\u0026#39;Error!\u0026#39;, error.message)) }) \u0026lt;/script\u0026gt; Comme vous pouvez le voir, ce script utilise l\u0026rsquo;API [Fetch] (https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API), un mécanisme assez nouveau basé sur des promesses pour effectuer des requêtes sur le web. Il effectue une requête \u0026ldquo;POST\u0026rdquo; sur l\u0026rsquo;URL de votre script et utilise FormData pour transmettre nos données comme paramètres d\u0026rsquo;URL.\nComme Fetch et FormData ne sont pas entièrement pris en charge, vous voudrez probablement inclure leurs polyfills respectifs. Voir section #8.\n Les balises \u0026lt;html\u0026gt;, \u0026lt;head\u0026gt; et body font partie d\u0026rsquo;une poignée de balises optionnelles, mais comme les règles relatives à l\u0026rsquo;analyse d\u0026rsquo;une page par le navigateur sont assez compliquées, vous ne voudrez probablement pas les omettre sur les vrais sites web.\n 7. Ajout de données supplémentaires dans le formulaire Pour saisir des données supplémentaires, il vous suffit de créer de nouvelles colonnes dont les titres correspondent exactement aux valeurs nom de vos entrées de formulaire. Par exemple, si vous voulez ajouter des entrées de prénom et de nom, vous leur donnerez des valeurs de nom comme ça :\n\u0026lt;form name=\u0026#34;submit-to-google-sheet\u0026#34;\u0026gt; \u0026lt;input name=\u0026#34;email\u0026#34; type=\u0026#34;email\u0026#34; placeholder=\u0026#34;Email\u0026#34; required\u0026gt; \u0026lt;input name=\u0026#34;firstName\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;First Name\u0026#34;\u0026gt; \u0026lt;input name=\u0026#34;lastName\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Last Name\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; Créez ensuite de nouveaux en-têtes avec les valeurs exactes, sensibles à la casse, des noms :\n    A B C D \u0026hellip;     1 timestamp email firstName lastName     8. Polyfills connexes Certains de ces éléments ne sont pas encore totalement pris en charge par les navigateurs ou ne fonctionnent pas sur les anciens. Voici quelques options de polyfill à utiliser pour une meilleure prise en charge.\n Promise Polyfill Fetch Polyfill FormData Polyfill  Étant donné que la polyfill FormData est publiée sous forme de paquet Node et doit être compilée pour que les navigateurs puissent fonctionner, une bonne option pour les inclure consiste à utiliser Browserify\u0026rsquo;s CDN appelé wzrd.in. Ce service compile, miniaturise et met à notre disposition la dernière version de ces scripts.\nVous devez vous assurer de leur chargement avant que le script principal ne traite la soumission du formulaire, par exemple\n\u0026lt;script src=\u0026#34;https://wzrd.in/standalone/formdata-polyfill\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://wzrd.in/standalone/promise-polyfill@latest\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://wzrd.in/standalone/whatwg-fetch@latest\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const scriptURL = \u0026#39;\u0026lt;SCRIPT URL\u0026gt;\u0026#39; const form = document.forms[\u0026#39;submit-to-google-sheet\u0026#39;] ... \u0026lt;/script\u0026gt; Vous avez des commentaires, des demandes ou des problèmes ? Veuillez créer une issue (https://github.com/jamiewilson/form-to-google-sheet/issues). Les relations publiques sont les bienvenues, mais n\u0026rsquo;hésitez pas à me faire part de vos idées avant de vous lancer dans un travail important. Je vous remercie !\nDes aricles similaires  Google Spreadsheets as a Database – INSERT with Apps Script form POST/GET submit method Step by step setup to send form data to Google Sheets Google Sheet Form Post How to Submit an HTML Form to Google Sheets…without Google Forms Send Email from a Static HTML Form using Google Apps Mail!  ","permalink":"/blog/soumettre-un-formulaire-html-a-google-sheet/","tags":["Google Apps","Javascript"],"title":"Envoyer un formulaire html directement à Google Sheets"},{"categories":["Growth Marketing"],"contents":"Le modèle du Marketeur T-shaped Comment découvrir la spécialisation en Marketing Digital faite pour vous. Le marketing constitue un domaine très vaste, ce qui signifie qu’il y a beaucoup de spécialisations possibles en tant que marketeur. On va à l’école pour découvrir plein de choses, mais c’est au fur et à mesure de sa carrière qu’on a l’occasion de se faire une idée du domaine dans lequel on est à l’aise et qui nous passionne le plus.\nLe modèle du T-shaped marketeur est là pour aider à identifier quelles sont les compétences fondamentales dont vous avez besoin et sur quels canaux vous pouvez devenir expert.\nLe modèle du T-Shaped marketeur Pour faire court, un marketer T-Shaped a les notions de base des compétences essentielles en marketing et se spécialise dans un ou plusieurs axes principaux.\nCe modèle est utile pour vous aider à vous faire une idée de votre profil et de celui de vos collaborateurs pour déterminer où se trouvent les points forts de votre équipe et quels domaines sont à externaliser.\nLe modèle du t-shaped marketeur a connu plusieurs versions et mises à jour, comme celles de Moz en 2013, Brian Balfour en 2013, de Buffer en 2017, Growth Tribe en 2018 et bien d’autres.\nAvec un niveau de complexité élevé, le modèle du marketer T-shaped ressemblerait à cela:\nPlus précisément, un diagramme T-Shaped type se rapproche de cet exemple fait par Buffer.\nMais en quoi consiste le travail d’un marketeur T-Shaped exactement? Commençons par découvrir le travail des marketeur T-Shaped dans le détail. Les Marketer t-shaped ont des compétences qui couvrent les notions essentielles du marketing, comme le montre la barre horizontale du T. La barre verticale du T représente le degré de connaissance et d’expertise dans des domaines spécifiques. Par exemple, une personne peut être définie par des branches plus longues en marketing de contenu et en branding, à l’inverse du web design.\nLe concept des Marketeurs T-shaped vient du monde de la RH. Le T décrit la portée et l’étendue des capacités de quelqu’un. Quel est votre degré de connaissance? Quelle est la qualité de votre connaissance?\nLes Marketeurs T-Shaped complètent les équipes avec toute personne capable de soutenir les autres membres et de prendre la direction des projets dans lesquels elle s’y connaît bien.\nLes fondements du marketing Les connaissances élémentaires des matières fondamentales en marketing constituent la partie la plus importante du modèle. Tout marketeur se doit de persévérer afin d\u0026rsquo;acquérir au minimum les bases de celles-ci.\nVoici les matières en marketing dont vous pouvez vous servir dans plusieurs voire dans tous les canaux.\nChoisir ses matières de prédilection Il s’agit de la partie amusante. Cela désigne tous les domaines que vous pouvez explorer pour générer de la croissance. Les choses changent très rapidement dans chaque canal. De nouveaux outils, de nouvelles manières de faire, de nouveaux résultats, de nouveaux règlements. On ne peut pas être un expert de chaque canal.\nAinsi, la plupart des Marketeurs T-Shaped ont de bonnes bases et une approche générale de chaque canal. Ils connaissent les avantages et les inconvénients de chaque canal mais sont aussi dotés d’une forte expertise dans certains.\nComment notre équipe marketing est-elle constituée? A Klipfolio, le directeur Marketing a choisi son équipe en se basant sur ces stratégies. Parmi nos 10 membres, nous disposons d’expertises clé en rédaction de site, optimisation du taux de conversion, en community management, SEO, graphisme, conception de site web et en automation.\nComment cela s’applique-t-il aux équipes marketing aujourd’hui? Une équipe marketing ”classique” apparaît très cloisonnée.\nDans le cadre d’un marketing moderne, un marketing efficace est permis par la capacité de chaque membre à avoir conscience du ”fit” des autres membres de l’équipe, et à cerner comment ce qu’ils apportent s’emboîte avec ce que les autres font.\nQuel que soit votre niveau, il est important de poser les bonnes questions qui prennent en compte les autres membres de l’équipe et les personnes d’autres équipes, et de le faire au bon moment. Cela garantit que chaque personne engagée sache que sa responsabilité dans la réalisation d’un projet marketing est clé.\nCeci est la raison pour laquelle nous avons besoin de marketeurs t-shaped ouverts d’esprit, opérationnels, de personnes prenant des décisions en ayant à l’esprit la variété de profils de l’équipe.\nNotre vision du marketeur t-shaped en 2020 Comme le marketing digital est en constante évolution, nous avons pensé qu’il était temps de s’essayer à une mise à jour du diagramme.\nNous avons essayé de lier plusieurs compétences fondamentales avec une chaîne d’expertise.\nPar exemple, une approche approfondie de la protection de la propriété intelectuelle peut vous permettre de vous forger une bonne expertise en SEO et dans le marketing de contenu.\nAlors qu’avoir des fondamentaux solides en analyse des données peut vous orienter vers l’automation ou le reporting. Certains de nos collaborateurs ont eu l’opportunité d’assister à la conférence sur le Growth Marketing à Toronto, et, après avoir discuté avec beaucoup de personnes de l’industrie pour arriver à un consensus, nous avons élaboré une nouvelle version en nous basant sur le modèle T-Shaped originel.\nAvoir de l’empathie pour ses clients Tout part d’une compréhension de vos consommateurs et de votre clientèle. C’est à ce moment là qu’entrent en jeu la psychlogie comportementale et la recherche.\nComprendre quelles sont les vraies sources d’insatisfaction à résoudre.\nCartographier et posséder tout le parcours client, l’engagement du consommateur, comprendre les déplacements des consommateurs et mettre en place des solutions qui résolvent ces problèmes d’insatisfaction.\nMiser sur le branding Drift prend position et affirme que ”les entreprises ne peuvent plus se différencier sur les caractéristiques dans un monde dans lequel l’offre est infinie. L’image de marque. L’image de marque est le seul vrai avantage marketing. ”\nSelon Bezos, le Branding est ”ce que les gens disent de vous quand vous n’êtes plus dans la même pièce”. Le branding est la création d’une dynamique et d’une histoire autour d’une entreprise, transcendant le produit et ses caractéristiques.\nVotre marque repose dans l’expérience globale que les personnes obtiennent en interagissant avec votre entreprise et votre produit. Ce n’est pas une chose en particulier mais un tout, fonctionnant à l’unisson.\nLe copywriting Aussi connue sous le nom de storytelling, c’est l’art d’élaborer l’offre pour un certain produit ou service en la façonnant de manière à ce qu’elle soit créative, intéressante et attrayante.\nC’est quelque chose que les machine ne sauront jamais bien réaliser. Il s’agit d’une tâche qui requiert un haut niveau d’abstraction, ce dont les machines ne sont pas capables, du moins pour l’instant…\nComprendre un produit, une clientèle, leurs sources de mécontentement et le formuler de manière à fidéliser les consommateurs. Cela a toujours été et restera un fondement du marketing qui peut être utilisé via tous les supports de communication.\nLe Funnel Marketing (tunnel de vente en français) Il est important de suivre les consommateurs du début à la fin de la “customer journey” (voir l\u0026rsquo;article growth hacking 101) et de définir/tracker clairement chaque étape.\nCela va nous permettre de baliser et de définir les points que conversions importants\nL’analyse de données Il s’agit du principe du test A/B. C’est le processus d’analyse, de transformation et de modélisation des données qui a pour but de collecter des perspectives, d’arriver à des conclusions éclairantes et de prendre une décision.\nHTML et Design Comprendre le code et faire un peu de wireframing c’est crucial.\nSans ces pré requis \u0026hellip;impossible de comprendre comment les services et les mécanismes du HTML et CSS, Javascript, jQuery, RESTful coopèrent.\nLe Leadership Le leadership c’est : mettre l’accent est mis sur la gestion des parties prenantes. Savoir former et rendre autonome ses employés La nécessité de montrer l’exemple.\nA vous de jouer maintenant J’espère que le modèle du marketeur T-Shaped vous a donné l\u0026rsquo;envie de réfléchir aux domaines en marketing dans lesquels vous êtes à l’aise et ceux qui vous tiennent le plus à cœur. Comprendre votre t-shape peut vous aider à déterminer les compétences dans lesquelles vous pouvez vous perfectionner et la direction à prendre pour progresser.\n","permalink":"/blog/c-est-quoi-le-t-shape-marketing/","tags":["T-Shape","Growth"],"title":"C'est quoi le T-shape marketing ?"},{"categories":["SEO"],"contents":"Le marketing digital dispose d’une pléthore d’outils pour rechercher et extraire des données. L’un des plus puissants d’entre eux mais qui reste trop souvent ignoré est l’utilisation des REGEX.\nConsultants SEO, Data analystes, Chercheurs,Community managers et spécialistes du marketing numérique en tout genre négligent parfois l’étendue de la puissance des expressions régulières.\nCet article définit ce que sont les Regex et leurs utilités lorsqu’elles sont combinées avec certains web crawlers, au travers de cinq cas pratiques.\nLes REGEX, qu’est ce que c’est? Les Regex (expressions régulières) , sont un outil de pattern matching.\nElles sont un élément incontournable pour les moteurs de recherche, les logiciels de search \u0026amp; replace, et sont une composante native ou complémentaire de nombreux langages de programmation.\nCombiner la méthode \u0026ldquo;find\u0026rdquo; avec un web crawler est un atout puissant tant pour identifier les erreurs que pour extraire des données.\nCette fonctionnalité est encore relativement nouvelle dans les outils pour SEO sur le marché.\nL\u0026rsquo;extraction personnalisée en utilisant des regex ( Xpath ou CSSpath), n\u0026rsquo;a été ajoutée à Screaming Frog SEO Spider que dans la version de juillet 2015.\nJusqu\u0026rsquo;à cette date, SEOTools pour Excel était peut-être l\u0026rsquo;outil le plus accessible pour l\u0026rsquo;extraction par regex - mais seulement lorsqu\u0026rsquo;il était combiné avec un crawler comme Screaming Frog pour collecter d\u0026rsquo;abord les URL à analyser.\nGoogle Sheets a aussi depuis longtemps la possibilité d\u0026rsquo;utiliser l\u0026rsquo;extraction par Regex mais, comme SEOTools pour Excel, il faut d\u0026rsquo;abord parcourir le site avec un web crawler.\nAprès que Screaming Frog ait déployé sa fonction d\u0026rsquo;extraction personnalisée par Regex en 2015, d’autres outils de référencement de niveau \u0026ldquo;entreprise\u0026rdquo; tel que BrightEdge et Conductor ont réagi en ajoutant des fonctions similaires.\nMalgré la présence désormais généralisée de l’extraction de données par Regex, la puissance de cette fonctionnalité est encore largement ignorée par de nombreux professionnels du web.\nComment apprendre les REGEX : Plongeons dans le vif du sujet avec une combinaison de Screaming Frog et d\u0026rsquo;outils comme :\n  RegExr\n  Regular Expressions 101\n  Txt2re\n  Build RegEx\n  Vous pouvez reprendre les cas d\u0026rsquo;utilisation ci-dessous, ou pratiquer sur vos propres projets.\nCas pratiques avec REGEX :\nCi dessous quelques exemples d’utilisation des Regex dont vous pouvez vous inspirer en ré-utilisant les formules.\n1. Extraire des métadonnées (ou toute autre donnée) de n\u0026rsquo;importe quel site web Regardons le site ESPN.\nPrenons pour l’exemple cette histoire sur la star de la NBA Blake Griffin.\nEn consultant le code source de cette page, nous localisons un type de métadonnées propice à analyser (des métadonnées similaires se trouvent sur la plupart des grands sites web) :\n{ \u0026#34;omniture\u0026#34;: { \u0026#34;columnist\u0026#34;: \u0026#34;lowe_zach\u0026#34;, \u0026#34;league\u0026#34;: \u0026#34;nba\u0026#34;, \u0026#34;countryRegion\u0026#34;: \u0026#34;en-us\u0026#34;, \u0026#34;hier1\u0026#34;: \u0026#34;nba:story\u0026#34;, \u0026#34;section\u0026#34;: \u0026#34;nba\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;espn.com\u0026#34;, \u0026#34;pageName\u0026#34;: \u0026#34;nba:story\u0026#34;, \u0026#34;storyInfo\u0026#34;: \u0026#34;22258759+zach-lowe-blake-griffin-trade-future-la-clippers-detroit-pistons\u0026#34;, \u0026#34;sections\u0026#34;: \u0026#34;nba:story\u0026#34;, \u0026#34;site\u0026#34;: \u0026#34;espn\u0026#34;, \u0026#34;premium\u0026#34;: \u0026#34;premium-no\u0026#34;, \u0026#34;convrSport\u0026#34;: \u0026#34;basketball\u0026#34;, \u0026#34;pageURL\u0026#34;: \u0026#34;www.espn.com/nba/story/_/id/22258759/zach-lowe-blake-griffin-trade-future-la-clippers-detroit-pistons\u0026#34;, \u0026#34;lang\u0026#34;: \u0026#34;en_us\u0026#34;, \u0026#34;prop35\u0026#34;: \u0026#34;2018-01-30\u0026#34;, \u0026#34;contentType\u0026#34;: \u0026#34;story\u0026#34;, \u0026#34;sport\u0026#34;: \u0026#34;basketball\u0026#34;, \u0026#34;account\u0026#34;: \u0026#34;wdgespcom\u0026#34;, \u0026#34;siteType\u0026#34;: \u0026#34;full\u0026#34;, \u0026#34;prop58\u0026#34;: \u0026#34;isIndex=false\u0026#34; } Le but étant d’identifier un modèle de données récurrent et intéressant à analyser. Une fois identifié, nous pouvons commencer à construire la regex pour extraire ces données.\nCopions simplement le code ainsi que quelques lignes avant et après dans regexr.com pour commencer à travailler sur quelques formules :\nSupposons que j’audit le site d\u0026rsquo;ESPN et que je veux obtenir une liste de tous leurs articles avec auteurs et dates. Il me suffit de lancer le crawler avec ces 2 expressions régulières en les incluant dans le filtre d\u0026rsquo;extraction personnalisée de Screaming Frog:\n\u0026quot;columnist\u0026quot;:\u0026quot;(.*?)\u0026quot; \u0026quot;prop35\u0026quot;:\u0026quot;(.*?)\u0026quot; Et ça marche ! Je peux parcourir une liste d\u0026rsquo;URL d\u0026rsquo;ESPN.com, ou parcourir tout le site, et en extraire les informations qui seront essentielles pour mon audit de contenu.\n2. Trademark Enforcement ( Droit des marques) Le symbole d’une marque déposée (®) est un symbole qui doit généralement apparaître juste après le nom de la marque déposée, dès sa première utilisation sur une page web.\nPrenons par exemple \u0026ldquo;ITIL®\u0026rdquo; - dont le sigle signifie IT Infrastructure Library, une marque déposée appartenant à Axelos.\nDe nombreux sites web qui parlent d\u0026rsquo;ITIL omettent d\u0026rsquo;inclure le symbole de la marque déposée lorsqu\u0026rsquo;ils font référence à ce concept.\nL\u0026rsquo;un de ces sites, Cherwell.com, parle d\u0026rsquo;ITIL mais n\u0026rsquo;inclut pas toujours le symbole de la marque déposée.\nEn utilisant deux Regex basiques, nous pouvons facilement trouver les URL dans lesquelles Cherwell devrait envisager d\u0026rsquo;ajouter le symbole de marque déposée :\nDans ce cas, nous avons repéré quatre URL qui comportent le bon symbole - et beaucoup d\u0026rsquo;autres qui ne le comportent pas.\nCet exemple montre à quel point une simple chaîne de regex peut être puissante.\nCi dessous les Regex utilisées :\nITIL® ITIL(?!®) 3. Changer le nom des produits ou rechercher une morphologie de caractères incorrecte Par exemple en 2017, IBM a officiellement changé le nom de \u0026ldquo;DB2\u0026rdquo; en \u0026ldquo;Db2\u0026rdquo;\nLe caractère \u0026ldquo;b\u0026rdquo; est devenu minuscule.\nLe nom DB2 a été utilisé pour la première fois en 1983, il y a donc probablement d\u0026rsquo;innombrables endroits sur le web qui gardent le nom incorrect de DB2.\nNe pas utiliser les regex et s\u0026rsquo;adapter à une morphologie changeante, c\u0026rsquo;est subir des erreurs fréquentes ou de ne pas trouver un mot ou ou le nom d\u0026rsquo;un produit qui change.\nEn utilisant l\u0026rsquo;expression régulière suivante, nous sommes capables d\u0026rsquo;identifier toutes les instances incorrectes de \u0026ldquo;DB2\u0026rdquo;, \u0026ldquo;db2\u0026rdquo; ou \u0026ldquo;dB2\u0026rdquo; :\nLa Regex :\n(db|DB|dB)2 4. Trouver des fichiers téléchargeable sur les sites communautaires Jive Software est peut-être la plate-forme communautaire N°1 pour les entreprises. Elle est utilisée par des sociétés comme Cisco, ServiceNow, Adobe, BMC Software, McAfe, Wiley et bien d\u0026rsquo;autres grandes entreprises.\nVous pouvez trouver d\u0026rsquo;autres exemples d\u0026rsquo;entreprises utilisant la plateforme Jive avec une requête Google, tel que :\nhttps://www.google.com/search?q=inurl:hosted.jivesoftware.com\nJive, tout comme WordPress ou d\u0026rsquo;autres plateformes de publication, présente certains patterns propre à ce genre de structure.\nPar exemple tous les téléchargements vers la plate-forme, lorsqu\u0026rsquo;ils sont visualisés à partir du document ou du blog sur lequel ils sont téléchargés, comprennent le lien suivant :\n\u0026lt;a class=\u0026#34;j-attachment-icon\u0026#34; href=\u0026#34;/servlet/JiveServlet/download/xxx/filename.ext\u0026#34;\u0026gt; Par conséquent, nous pouvons utiliser la Regex suivante pour extraire toutes les URL de pièces jointes de n\u0026rsquo;importe quel site communautaire hébergé sur Jive :\n\u0026quot;j-attachment-icon\u0026quot; href=\u0026quot;(.*?)\u0026quot; Pour voir comment ça fonctionne, voici les résultats d\u0026rsquo;un bref scan du site https://community.servicenow.com :\n5. Contrôler l’orthographe Fort heureusement, la plupart des éditeurs vérifient l\u0026rsquo;orthographe dans Word \u0026amp; autres avant de publier leurs contenu, mais les fautes d\u0026rsquo;orthographe les plus courantes passent encore inaperçues.\nPas d’inquiétudes :\nRegex + Screaming Frog peut vous aider à identifier les mots couramment mal orthographiés.\nLe seul inconvénient est que vous devez connaître/produire la liste de ces mots.\nCertains mots sont susceptible d’êtres mal orthographié dans n’importe quelle langue.\nIl existe également des mots spécifiques à un secteur ou à une marque qui doivent être inclus dans la liste des fautes d\u0026rsquo;orthographe.\nVoici une simple Regex pour faire matcher les mots anglais les plus couramment mal orthographiés aux États-Unis :\n(?i)accomodate | accomodation | acheive | accross | agressive | agression | apparantly | appearence | arguement | assasination | basicly | begining | beleive | belive | bizzare | buisness | calender | Carribean | cemetary | chauffer | collegue | comming | commitee | completly | concious | curiousity | definately | dilemna | dissapear | dissapoint | ecstacy | embarass | enviroment | existance | Farenheit | familar | finaly | florescent | foriegn | forseeable | fourty | foward | freind | futher | jist | glamourous | goverment | gaurd | happend | harrass | harrassment | honourary | humourous | idiosyncracy | immediatly | incidently | independant | interupt | irresistable | knowlege | liase | liason | lollypop | millenium | millenia | Neandertal | neccessary | noticable | ocassion | occassion | occured | occuring | occurance | occurence | pavillion | persistant | pharoah | peice | politican | Portugese | posession | prefered | prefering | propoganda | publically | realy | recieve | refered | refering | religous | rember | remeber | resistence | sence | seperate | seige | succesful | supercede | suprise | tatoo | tendancy | therefor | threshhold | tommorow | tommorrow | tounge | truely | unforseen | unfortunatly | untill | wierd | whereever | wich Utilisons Screaming Frog à nouveau et voyons si nous pouvons localiser une de ces erreurs sur CNN :\nEn un rien de temps, nous avons déjà repéré une mauvaise orthographe du mot \u0026ldquo;Fahrenheit\u0026rdquo;\nEn résumé On peut trouver des schémas de données partout sur le web. En utilisant les Regex, vous pouvez les localiser et en extraire les données associés.\nLes REGEX ne sont un outil simple à appréhender , mais une fois maîtrisée, vous allez trouver des utilisations infinies pour cette technologie omniprésente sur le web. C\u0026rsquo;est dans cet esprit que Jamie Zawinski a déclaré :\n“Certaines personnes, lorsqu\u0026rsquo;elles sont confrontées à un problème, pensent \u0026ldquo;Je sais, je vais utiliser des expressions régulières\u0026rdquo;. Maintenant, ils ont deux problèmes.”\n","permalink":"/blog/utiliser-les-regex-pour-le-seo-et-l-extraction-de-donnees/","tags":["REGEX","Python","Screaming Frog"],"title":"Comment utiliser les REGEX pour le SEO et l’extraction de données"},{"categories":["SEO"],"contents":"L\u0026rsquo;article développe sous python et javascript une solution d\u0026rsquo;automatisation de synthèse de texte en prenant en compte des dernieres solutions NLU/NLP.\nLe but étant d\u0026rsquo;étudier le CTR d\u0026rsquo;une méta-description créée automatiquement vs une méta description optimisée humainement.\nEn ces temps difficiles, il est plus important que jamais d\u0026rsquo;optimiser son temps de travail en dépensant le moins de ressources possible.\nUne tâche peu prisée et souvent négligée par les SEO est la rédaction de titres et de méta descriptions convaincantes.\nEn particulier lorsque le site compte des milliers ou des millions de pages.\nIl est difficile de faire l'effort quand on ne sait pas si la récompense en vaudra la peine.\nDans cette rubrique, vous allez apprendre à utiliser les dernières nouveautés en matière de NLU / NLP pour produire automatiquement des titres et des méta descriptions optimisées.\nCette tactique d\u0026rsquo;automatisation passionnante, nous allons l\u0026rsquo;aborder de manière pratique sur de Google Sheets.\nNous apprendrons à mettre en œuvre cette fonctionnalité avec un minimum de code Python et JavaScript.\nVoici notre méthodologie:   Nous allons mettre en œuvre et évaluer quelques modèles récents de synthétisation de texte dans Google Colab\n  Nous allons nous servir d\u0026rsquo;un des modèles à partir d'une fonction Google Cloud que nous pouvons facilement appeler depuis Apps Script et Google Sheets\n  Nous scraperons le contenu des pages directement à partir de Google Sheets et le résumerons grâce à notre fonction personnalisée\n  Nous déploierons nos titres et méta descriptions générés pour les tester dans Cloudflare en utilisant RankSense\n  Nous créerons une autre fonction Google Cloud pour déclencher l \u0026lsquo;indexation automatique dans Bing\n   Présentation de la librairie \u0026ldquo;Hugging Face Transformers\u0026rdquo; Hugging Face Transformers est une bibliothèque populaire parmi les chercheurs en IA.\nElle fournit une interface unifiée et simple à utiliser pour les dernières recherches sur le langage naturel.\nIl pas très important de savoir que cette librairie ait été codée à l'aide de Tensorflow (Google\u0026rsquo;s Deep Learning framework) ou Pytorch (Facebook\u0026rsquo;s framework).\nBien que la bibliothèque transformers offre un code plus simple, elle n\u0026rsquo;est pas aussi facile à utiliser pour les utilisateurs finaux que Ludwig.\nCe point a changé récemment avec l\u0026rsquo;introduction de transformers pipelines.\nLes pipelines encapsulent de nombreux cas d\u0026rsquo;utilisation courante que l’on peut retrouver en NLP. Le tout avec un code minimal.\nIls offrent également une grande flexibilité dans l'utilisation du modèle sous-jacent.\nEn plusieurs étapes nous établirons des \u0026ldquo;transformer pipelines\u0026rdquo;, pratiques et qui contiennent dernières nouveautés NLP.\nNous emprunterons un peu de code aux exemples présentés sur ce notebook.\nBART Facebook Lors de l'annonce du projet BART, Mike Lewis, chercheur sur Facebook, a fait état de leur capacité de synthétisation de texte.\nQui sont à vrai dire, vraiment impressionnantes !\n Nous sommes ravis de partager notre travail sur BART, une méthode de pré-formation des modèles seq2seq par denoising .\n  BART surpasse les travaux antérieurs sur un ensemble de tâches de génération (résumé/dialogue/assurance qualité), tout en obtenant des performances similaires à celles de RoBERTa sur SQuAD/GLUE\n  J'ai trouvé la performance de synthèse étonnamment bonne\n  BART semble en effet être capable de combiner des informations provenant de l'ensemble d'un document avec des connaissances de fond pour produire des résumés très pertinents. Voici quelques exemples typiques :\n Voyons maintenant à quel point il est facile de reproduire les résultats de leur travail à l'aide de transformers pipelines.\nTout d'abord, installons la bibliothèque dans un nouveau Google Colab.\nVeillez à sélectionner le temps d'exécution du GPU.\ncolab$ !pip install transformers Ensuite, ajoutons à cela le code du pipeline.\nfrom transformers import pipeline # utiliser Bart dans pytorch bart_summarizer = pipeline(\u0026#34;summarization\u0026#34;) Voici l'exemple de texte que nous allons résumer.\nTEXT_TO_SUMMARIZE = \u0026#34;\u0026#34;\u0026#34; New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \u0026#34;I do\u0026#34; five more times, sometimes only within two weeks of each other. In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \u0026#34;first and only\u0026#34; marriage. Barrientos, now 39, is facing two criminal counts of \u0026#34;offering a false instrument for filing in the first degree,\u0026#34; referring to her false statements on the 2010 marriage license application, according to court documents. Prosecutors said the marriages were part of an immigration scam. On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further. After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say. Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages. Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted. The case was referred to the Bronx District Attorney\\\u0026#39;s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\\u0026#39;s Investigation Division. Seven of the men are from so-called \u0026#34;red-flagged\u0026#34; countries, including Egypt, Turkey, Georgia, Pakistan and Mali. Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force. If convicted, Barrientos faces up to four years in prison. Her next court appearance is scheduled for May 18. \u0026#34;\u0026#34;\u0026#34; Voici le code pour synthétiser et le résumé qui en résulte :\nsummary = bart_summarizer(TEXT_TO_SUMMARIZE, min_length=50, max_length=250) print(summary) #Output:  [ { \u0026#39;summary_text\u0026#39;: \u0026#39;Liana Barrientos has been married 10 times, sometimes within two weeks of each other. Prosecutors say the marriages were part of an immigration scam. She is believed to still be married to four men, and at one time, she was married to eight at once.\u0026#39; } ] [!] J\u0026rsquo;ai précisé que le résumé généré ne devait pas comporter moins de 50 caractères et au maximum 250.\nCeci est très utile pour contrôler le type de synthèse que vous souhaitez effectuer : titres ou méta descriptions. Maintenant, regardez la qualité du résumé généré, et nous n\u0026rsquo;avons tapé que quelques lignes de code Python. Super cool!\n\u0026#39;Lana Barrientos has been married 10 times, sometimes within two weeks of each other. Prosecutors say the marriages were part of an immigration scam. She is believed to still be married to four men, and at one time, she was married to eight at once.\u0026#39; print(len(summary[0][\u0026#34;summary_text\u0026#34;])) #Output: 249 LeT5 de Google Un autre modèle de référence dans le domaine est le Text-to-Text Transfer Transformer, ou T5.\nL'une des réalisations impressionnantes de ce model est que ses performances se sont rapprochées du niveau de référence au niveau humain dans le SuperGLUE leaderboard.\n\u0026ldquo;Le modèle linguistique T5 (Text-To-Text Transfer Transformer) de Google a établi un nouveau record et se rapproche beaucoup de l\u0026rsquo;humain sur le benchmark SuperGLUE.\u0026quot;\nCode:\nhttps://github.com/google-research/text-to-text-transfer-transformer\n IMAGE\n C\u0026rsquo;est remarquable car les tâches de NLP dans SuperGLUE sont conçues pour être faciles pour les humains mais difficiles pour les machines.\nGoogle a récemment publié un résumé avec tous les détails du model pour les personnes moins familières des articles sur la recherche.\nLeur idée principale était d\u0026rsquo;essayer toutes les idées populaires de la NLP sur un nouvel ensemble de données de formation massive qu\u0026rsquo;ils appellent C4 (Colossal Clean Crawled Corpus).\nJe sais, les chercheurs en IA aiment s'amuser à nommer leurs inventions.\nUtilisons un autre transformers pipeline pour résumer le même texte, mais cette fois en utilisant T5 comme modèle sous-jacent.\nt5_summarizer = pipeline(\u0026#34;summarization\u0026#34;, model=\u0026#34;t5-base\u0026#34;, tokenizer=\u0026#34;t5-base\u0026#34;) summary = t5_summarizer(TEXT_TO_SUMMARIZE, min_length=50, max_length=250) Voici le texte résumé.\n[ { \u0026#34;summary_text\u0026#34;: \u0026#34;in total, barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002 . she is believed to still be married to four men, and at one time, she was married to eight men at once .\u0026#34; } ] Ce résumé est également de très grande qualité.\nMais j\u0026rsquo;ai décidé d'essayer le modèle T5, le plus large, qui est également disponible sous forme de pipeline, pour voir si la qualité pouvait être améliorée.\nt5_summarizer_larger = pipeline(\u0026#34;summarization\u0026#34;, model=\u0026#34;t5-large\u0026#34;, tokenizer=\u0026#34;t5-large\u0026#34;) Je n'ai pas du tout été déçu.\nUn résumé vraiment impressionnant !\n[ { \u0026#39;summary_text\u0026#39;: \u0026#39;Liana barrientos has been married 10 times . nine of her marriages occurred between 1999 and 2002 . she is believed to still be married to four men, and at one time, she was married to eight men at once .\u0026#39; } ] Présentation des Cloud Fonctions Maintenant que nous disposons d'un code capable de résumer efficacement le contenu d'une page, il nous faut un moyen simple de l'exploiter sous forme d'API.\nDans un article précédent, j'ai utilisé le service Ludwig, mais comme nous n'utilisons pas Ludwig ici, nous allons adopter une approche différente : les Cloud Functions.\nLes fonctions \u0026ldquo;cloud\u0026rdquo; \u0026amp; équivalences \u0026quot;serverless\u0026quot; sont sans doute le moyen le plus simple d'obtenir du code côté serveur pour du déploiement.\nOn les appelle \u0026ldquo;serverless\u0026rdquo; parce qu\u0026rsquo;il n'est pas nécessaire de mettre à disposition des serveurs web ou des VM chez hébergeurs.\nElles simplifient considérablement l\u0026rsquo;expérience de déploiement,nous allons le voir.\nDéploiement d\u0026rsquo;une fonction Cloud \u0026ldquo;Hello World\u0026rdquo;\nNous n'avons pas besoin de quitter Google Colab pour déployer notre premier test.\nTout d'abord, connectez-vous à votre compte \u0026ldquo;Google Compute\u0026rdquo;.\n colab$ !gcloud auth login \\--no-launch-browser Ensuite, mettez en place un projet par défaut.\n colab$ !gcloud config set project project-name Nous allons écrire notre fonction de test dans un fichier nommé main.py\n%%writefile main.py def hello_get(request): \u0026#34;\u0026#34;\u0026#34;HTTP Cloud Function. Args: request (flask.Request): The request object. Returns: The response text, or any set of values that can be turned into a Response object using `make_response` . \u0026#34;\u0026#34;\u0026#34; return \u0026#39;Hello World!\u0026#39; Nous pouvons déployer cette fonction à l'aide de cette commande.\ncolab $ !gcloud functions deploy hello_get --runtime python37 --trigger-http --allow-unauthenticated Après quelques minutes, nous obtenons les détails de notre nouveau service API.\n availableMemoryMb: 256 entryPoint: hello_get httpsTrigger: url: https://xxx.cloudfunctions.net/hello_get ingressSettings: ALLOW_ALL labels: deployment-tool: cli-gcloud name: projects/xxx/locations/us-central1/functions/hello_get runtime: python37 serviceAccountEmail: xxxx sourceUploadUrl: xxxx status: ACTIVE timeout: 60s updateTime: '2020-04-06T16:33:03.951Z' versionId: '8' C\u0026rsquo;est tout !\nNous n\u0026rsquo;avons pas eu besoin de mettre en place de VM \u0026amp; co.\nNous pouvons le tester en ouvrant l\u0026rsquo;URL fournie et en obtenant le texte \u0026ldquo;Hello World !\u0026rdquo; comme réponse dans le navigateur.\nDéployer notre cloud fonction\nEn théorie, nous devrions être capables d\u0026rsquo;envelopper notre pipeline de text transformers dans une fonction et de suivre les mêmes étapes pour déployer un service API.\nCependant, j\u0026rsquo;ai dû surmonter plusieurs difficultés pour que cela fonctionne.\nNotre premier (et plus difficile) problème a été l\u0026rsquo;installation de la bibliothèque transformers pour commencer.\nHeureusement, il est simple d\u0026rsquo;installer des paquets tiers à utiliser dans les Python-based Cloud Functions.\nIl vous suffit de créer une fichier standard requirements.txt comme ça:\n%%writefile requirements.txt\\ transformers==2.8.0  Malheureusement, ça échoue car transformers nécessite pour fonctionner de Pytorch ou de Tensorflow.\nIls sont tous deux installés par défaut dans Google Colab, mais doivent être spécifiés dans l\u0026rsquo;environnement Cloud Functions.\nPar défaut, transformers utilise Pytorch, et lorsque je l\u0026rsquo;ai mis en priorité, ça a retourné une erreur qui m'a conduit à ce thread utile de Stack Overflow.]\nJe l\u0026rsquo;ai fait fonctionner avec ce fichier requirement.txt mis à jour.\n%%writefile requirements.txt https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp37-cp37m-linux_x86_64.whl transformers==2.8.0 Le défi suivant était l'énorme besoin de mémoire des models et les limites des cloud fonctions.\nJ'ai d'abord testé des fonctions en utilisant des pipelines plus simples comme celui du NER, NER signifie Name Entity Recognition.\nJe le teste d\u0026rsquo;abord dans le Colab notebook.\nfrom transformers import pipeline nlp_token_class = None def ner_get(request): global nlp_token_class #run once if nlp_token_class is None: nlp_token_class = pipeline(\u0026#39;ner\u0026#39;) result = nlp_token_class(\u0026#39;Hugging Face is a French company based in New-York.\u0026#39;) return result Voici le listing détaillé des réponses .\n[{\u0026#39;entity\u0026#39;: \u0026#39;I-ORG\u0026#39;, \u0026#39;score\u0026#39;: 0.9970937967300415, \u0026#39;word\u0026#39;: \u0026#39;Hu\u0026#39;}, {\u0026#39;entity\u0026#39;: \u0026#39;I-ORG\u0026#39;, \u0026#39;score\u0026#39;: 0.9345749020576477, \u0026#39;word\u0026#39;: \u0026#39;##gging\u0026#39;}, {\u0026#39;entity\u0026#39;: \u0026#39;I-ORG\u0026#39;, \u0026#39;score\u0026#39;: 0.9787060022354126, \u0026#39;word\u0026#39;: \u0026#39;Face\u0026#39;}, {\u0026#39;entity\u0026#39;: \u0026#39;I-MISC\u0026#39;, \u0026#39;score\u0026#39;: 0.9981995820999146, \u0026#39;word\u0026#39;: \u0026#39;French\u0026#39;}, {\u0026#39;entity\u0026#39;: \u0026#39;I-LOC\u0026#39;, \u0026#39;score\u0026#39;: 0.9983047246932983, \u0026#39;word\u0026#39;: \u0026#39;New\u0026#39;}, {\u0026#39;entity\u0026#39;: \u0026#39;I-LOC\u0026#39;, \u0026#39;score\u0026#39;: 0.8913459181785583, \u0026#39;word\u0026#39;: \u0026#39;-\u0026#39;}, {\u0026#39;entity\u0026#39;: \u0026#39;I-LOC\u0026#39;, \u0026#39;score\u0026#39;: 0.9979523420333862, \u0026#39;word\u0026#39;: \u0026#39;York\u0026#39;}] Ensuite, je peux simplement ajouter un %%writefile main.py pour créer un fichier Python que je vais utiliser pour déployer la fonction.\nQuand j’ai regardé les logs pour savoir pourquoi les appels API avaient échoué, j\u0026rsquo;ai vu que le besoin en mémoire était un gros problème.\nMais heureusement, vous pouvez facilement outrepasser la limite par défaut de 250M et augmenter le délai d\u0026rsquo;exécution en utilisant cette commande.\ncolab $ !gcloud functions deploy ner_get --memory 2GiB --timeout 540 --runtime python37 --trigger-http --allow-unauthenticated Je précise que la mémoire doit être configurée sur 2 Go et que le délai d\u0026rsquo;exécution est de 9 minutes. Ce qui doit faciliter au téléchargement initial du modèle qui fait plusieurs gigaoctets.\nMa technique pour accélérer les appels successifs d’une même fonction Cloud, c’est la mise en cache du modèle téléchargé. en mémoire à l\u0026rsquo;aide d\u0026rsquo;une variable globale et à vérifier si elle existe avant de recréer le pipeline.\nAprès avoir testé les fonctions BART et T5 et réglé un petit modèle T5 qui correspond bien aux exigences de mémoire et de délai d\u0026rsquo;attente des fonctions Cloud. Voici le code de cette fonction.\n%%writefile main.py from transformers import pipeline nlp_t5 = None def t5_get(request): global nlp_t5 #run once if nlp_t5 is None: nlp_t5 = pipeline(\u0026#39;summarization\u0026#39;, model=\u0026#34;t5-small\u0026#34;, tokenizer=\u0026#34;t5-small\u0026#34;) TEXT_TO_SUMMARIZE = \u0026#34;\u0026#34;\u0026#34; New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York... \u0026#34;\u0026#34;\u0026#34; result = nlp_t5(TEXT_TO_SUMMARIZE) return result[0][\u0026#34;summary_text\u0026#34;] Et voici le code pour la déployer :\ncolab $ !gcloud functions deploy t5_get --memory 2GiB --timeout 540 --runtime python37 --trigger-http --allow-unauthenticated Le problème avec cette fonction c’est que le texte à résumer est codé en dur.\nMais nous pouvons facilement corriger cela avec les modifications suivantes.\n%%writefile main.py from transformers import pipeline nlp_t5 = None def t5_post(request): global nlp_t5 #run once if nlp_t5 is None: #small model to avoid memory issue nlp_t5 = pipeline(\u0026#39;summarization\u0026#39;, model=\u0026#34;t5-small\u0026#34;, tokenizer=\u0026#34;t5-small\u0026#34;) #Get text to summarize from POST request content_type = request.headers[\u0026#39;content-type\u0026#39;] if content_type == \u0026#39;application/x-www-form-urlencoded\u0026#39;: text = request.form.get(\u0026#39;text\u0026#39;) result = nlp_t5(text) return result[0][\u0026#34;summary_text\u0026#34;] else: raise ValueError(\u0026#34;Unknown content type: {}\u0026#34;.format(content_type)) return \u0026#34;Failure\u0026#34; Je m\u0026rsquo;assure simplement que le type de contenu est form url-encoded, et je lis le paramètre texte à partir des données du formulaire.\nJe peux facilement tester cette fonction dans Colab en utilisant le code suivant.\nimport requests url = \u0026#34;https://xxx.cloudfunctions.net/hello_post\u0026#34; data = {\u0026#34;text\u0026#34;: text[:100]} requests.post(url, data).text Comme tout fonctionne comme prévu, je fais en sorte que cela fonctionne aussi dans Google Sheets. Faire un call de notre service de synthèse de texte depuis Google Sheets Apps Script donne vraiment de super pouvoirs à Google Sheets.\nJ\u0026rsquo;ai pu apporter des modifications mineures à la fonction que j\u0026rsquo;ai créée pour la rendre utilisable pour la synthèse de texte.\nfunction getSummary(text){ payload = `text=${text}`; payload = encodeURI(payload); console.log(payload); var url = \u0026#34;https://xxx.cloudfunctions.net/t5_post\u0026#34;; var options = { \u0026#34;method\u0026#34; : \u0026#34;POST\u0026#34;, \u0026#34;contentType\u0026#34; : \u0026#34;application/x-www-form-urlencoded\u0026#34;, \u0026#34;payload\u0026#34; : payload, \u0026#39;muteHttpExceptions\u0026#39;: true }; var response = UrlFetchApp.fetch(url, options); var result = response.getContentText(); console.log(result); return result; } C\u0026rsquo;est tout.\n J\u0026rsquo;ai changé le nom de la variable d\u0026rsquo;entrée et l\u0026rsquo;URL de l\u0026rsquo;API.\nLe reste est identique à ce dont j\u0026rsquo;ai besoin pour soumettre une demande POST .\nNous effectuons des tests et vérifions les logs de la console pour nous assurer que tout fonctionne comme prévu. C\u0026rsquo;est le cas.\nL\u0026rsquo;une des grandes limites des Apps Script est que les fonctions personnalisées ne peuvent pas s\u0026rsquo;exécuter pendant plus de 30 secondes.\nEn pratique, cela signifie que je peux résumer un texte s\u0026rsquo;il comporte moins de 1 200 caractères, alors qu\u0026rsquo;en Colab/Python, j\u0026rsquo;ai testé des articles complets comportant plus de 10 000 caractères.\nUne autre approche qui devrait mieux fonctionner pour les textes plus longs est de mettre à jour le Google Sheet à partir d’un code Python.\nScraper le contenu des pages depuis Google Sheets Voici quelques exemples de l\u0026rsquo;exécution du code dans classeur.\nMaintenant que nous sommes en mesure de résumer le contenu des textes, voyons comment nous pouvons l\u0026rsquo;extraire directement des pages web publiques.\nGoogle Sheets comprend à cet effet une fonction puissante appelée IMPORTXML.\nIl nous suffit de fournir l\u0026rsquo;URL et un sélecteur XPath qui identifie le contenu que nous voulons extraire. Voici le code pour extraire le contenu d\u0026rsquo;une page Wikipédia.\n=IMPORTXML(\u0026quot;https://en.wikipedia.org/wiki/Moon_landing\u0026quot;, \u0026quot;//div/text()\u0026quot;) J\u0026rsquo;ai utilisé un sélecteur générique pour capturer tout le texte à l\u0026rsquo;intérieur des DIV.\nN\u0026rsquo;hésitez pas à jouer avec différents sélecteurs qui correspondent au contenu de votre page cible.\nBien que nous puissions obtenir le contenu de la page, il se répartit sur plusieurs lignes. Nous pouvons corriger cela avec une autre fonction, TEXTJOIN.\n=TEXTJOIN(\u0026quot; \u0026quot;, TRUE, IMPORTXML(\u0026quot;https://en.wikipedia.org/wiki/Moon_landing\u0026quot;, \u0026quot;//div/text()\u0026quot;)) Indexation rapide de nos nouvelles méta descriptions et titres dans Bing Alors, comment savoir si ces nouvelles metas sont plus performantes que celles écrites manuellement ou par rapport à l\u0026rsquo;absence totale de méta-données ?\nUn moyen sûr d\u0026rsquo;apprendre est d\u0026rsquo;effectuer un test en direct. Dans ce cas, il est essentiel d\u0026rsquo;indexer rapidement nos modifications.\n(article futur dessus prévu)\nCette approche nous limite à quelques centaines de pages. Une meilleure alternative est d\u0026rsquo;utiliser l\u0026rsquo;incroyable API d\u0026rsquo;indexation rapide de Bing, car nous pouvons demander l\u0026rsquo;indexation de 10 000 URL !\nC\u0026rsquo;est pas génial ? Comme nous serions principalement intéressés par la mesure du CTR, notre hypothèse est que si nous obtenons une CTR plus élevée dans Bing, la même chose se produira probablement dans Google et d\u0026rsquo;autres moteurs de recherche.\n“Un service permettant aux éditeurs d\u0026rsquo;obtenir des URL instantanément indexées dans Bing et de commencer réellement à les classer en quelques minutes mérite d\u0026rsquo;être envisagé. C\u0026rsquo;est comme de l\u0026rsquo;argent gratuit, pourquoi personne ne le prendrait ?”\nJe suis d\u0026rsquo;accord avec Roger. Voici le code pour le faire.\napi_key = \u0026#34;xxx\u0026#34; # Get your own API key from this URL https://www.bing.com/webmaster/home/api import requests def submit_to_bing(request): global api_key a1=\u0026#34;https://ssl.bing.com/\u0026#34; a2 =f\u0026#34;webmaster/api.svc/json/SubmitUrlbatch?apikey={api_key}\u0026#34; api_url = a1 + a2 print(api_url) #Replace for your site  url_list = [ \u0026#34;https://www.domain.com/page1.html\u0026#34;, \u0026#34;https://www.domain.com/page2.html\u0026#34;] data = { \u0026#34;siteUrl\u0026#34;: \u0026#34;http://www.domain.com\u0026#34;, \u0026#34;urlList\u0026#34;: url_list } r = requests.post(api_url, json=data) if r.status_code == 200: return r.json() else: return r.status_code Si la demande est acceptée, vous devez vous attendre à cette réponse.\n{\u0026#39;d\u0026#39;: None} Nous pouvons ensuite créer une autre fonction Cloud en l\u0026rsquo;ajoutant à notre fichier main.py et en utilisant la commande deploy comme auparavant\nTester nos métas générées dans Cloudfare Enfin, si votre site utilise le CDN Cloudflare, vous pouvez utiliser l\u0026rsquo;application RankSense pour exécuter ces changements à titre expérimental avant de les déployer sur le site.\nIl vous suffit de copier l\u0026rsquo;URL et les colonnes de résumé de texte dans une nouvelle feuille Google et de l\u0026rsquo;importer dans l\u0026rsquo;outil.\nLorsque vous publiez le test, vous pouvez choisir de programmer le changement et de spécifier une URL webhook. Une URL webhook permet aux applications et aux services de se parler dans le cadre de flux de travail automatisés.\nCopiez et collez l\u0026rsquo;URL de la fonction d\u0026rsquo;indexation de Bing dans le cloud. RankSense l\u0026rsquo;appellera automatiquement 15 minutes après que les changements se soient propagés dans Cloudflare.\n","permalink":"/blog/automatiser-la-creation-de-titres-et-de-meta-descriptions-avec-le-nlu-nlp-et-python/","tags":["NLU","NLP","Python","SEO"],"title":"SEO : Automatiser la création de titres et de méta-descriptions"}]